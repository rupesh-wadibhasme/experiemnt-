We apply a structured preprocessing pipeline to clean, normalize, and standardize the raw bank-statement data. Key steps include:

Validating required columns and removing duplicate transactions (based on Txn ID).

Cleaning categorical fields (Account, BU, Code, CashBookFlag) to handle blanks, inconsistent formats, and unknowns.

Deriving a unified timestamp (ts) using PostingDate as primary and ValueDate as fallback.

We engineer a set of stable, interpretable behavioural features:

Posting lag (days) – Payment timing behaviour.

Calendar features – Day-of-week, weekend flag, month, quarter.

Cashbook flag (derived) – Identifies internal/manual postings.

Same-amount-count-per-day – Captures repetitive transfers or structured entries.

Combo string = Account | BusinessUnit | TransactionCode for grouping behavioural patterns.

For each combo (using training data only), we compute robust statistics:

Median & IQR (log-space) of amounts → used to create y_norm, a combo-normalized amount.

MAD in INR → used for anomaly tolerance.
This produces a final feature matrix: tabular behavioural features + y_norm, enabling the model to learn normal behaviour per combo and detect deviations reliably and explainably.


Our model training pipeline uses a combo-aware autoencoder designed to learn normal behaviour for each (Account, BU, Code) combination. We split the data temporally: early months for training, tail segment for validation. Training uses weighted reconstruction loss, giving higher importance to the normalized amount feature (y_norm) and automatically down-weighting very large transactions and frequent combos. The autoencoder jointly learns:

Reconstruction of engineered behavioural features

A compact bottleneck representation capturing combo-specific patterns

Combo embeddings that encode differences between transaction groups

Validation Logic & Thresholding

Validation rows are used to compute reconstruction-error percentiles, forming anomaly thresholds.
Key components:

Global threshold (99th percentile) from all validation rows

Per-combo thresholds (where enough validation data exists)

Adaptive INR tolerance using MAD (Median Absolute Deviation) to ensure anomalies are flagged only when the INR difference is meaningful
A prediction is flagged anomalous only when both conditions hold:

Reconstruction error ≥ threshold

Amount deviation ≥ adaptive tolerance

Visualization Outputs (Training & Validation)

To support model interpretability, we generate multiple diagnostic plots:

Training vs Validation Loss Curve → shows convergence, early stopping, and overfitting protection.

Combo-Level Amount Drift Chart → plots training history amounts, alongside the test actual vs model-predicted amount.

y_norm Drift Chart → visualizes how far normalized test values deviate from the combo’s median & IQR bands.
These plots help validate whether the model is capturing realistic behavioural structure and why the flagged transactions deviate from learned patterns.
